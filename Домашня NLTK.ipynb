{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href = \"https://colab.research.google.com/github/flyingzaptop/pyton-colab/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%8888 8%D0%BD18. „https://colab.research.google.com/assets/colab-badge.svg” alt = \"Otwórz w colab\"/> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 1\nNapisz funkcję, która zwraca listę fraz z tekstu odpowiadającego określonego strupu. W razie potrzeby możesz dodać własne parametry.\n\nPrzetestuj funkcję na niektórych tekstach."
      ],
      "metadata": {
        "id": "D56Rwjf4EXC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_phrases(text, regex, tag_name):\n",
        "  \"\"\"\n",
        "  Reurn list of phrases from text that matches regex\n",
        "\n",
        "  Params:\n",
        "    text: str - original text\n",
        "    regex: str - regular expression\n",
        "    tag_name: str - tag name that is used in nltk tree\n",
        "\n",
        "  Return:\n",
        "    phrases: list[str] - list of phrases\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "G4Stff_zFQjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Напишіть функцію, яка повертає список фраз з тексту, які відповідають певному шоблону. При необхідності можете добавити власні параметри.\n",
        "# Протестуйте функцію на якомусь тексті.\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "def get_phrases(text, regex, tag_name):\n",
        "    phrases = []\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))):\n",
        "          if hasattr(chunk, 'label') and chunk.label() == tag_name:\n",
        "            phrase = \" \".join([c[0] for c in chunk])\n",
        "            if re.match(regex, phrase):\n",
        "                phrases.append(phrase)\n",
        "    return phrases\n",
        "\n",
        "\n",
        "# Test the function\n",
        "text = \"My name is John Doe and I live in New York. My phone number is 555-123-4567. I also like to eat pizza from Pizza Hut.\"\n",
        "regex = r\".*\"  # Match any phrase\n",
        "tag_name = \"PERSON\"\n",
        "phrases = get_phrases(text, regex, tag_name)\n",
        "print(phrases)  # Output: ['John Doe']\n",
        "\n",
        "regex = r\"\\d{3}-\\d{3}-\\d{4}\" # Match phone number\n",
        "tag_name = None # tag_name is not used in this case\n",
        "phrases = get_phrases(text, regex, tag_name)\n",
        "print(phrases) # Output: ['555-123-4567']\n",
        "\n",
        "# Download necessary NLTK data if you haven't already\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    nltk.data.find('chunkers/maxent_ne_chunker')\n",
        "    nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('maxent_ne_chunker')\n",
        "    nltk.download('words')"
      ],
      "metadata": {
        "id": "j8mmC6xeSk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "id": "FBzizhxPStkr",
        "outputId": "d7958b25-9e26-4e8f-d90a-93a9b87a1abb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_phrases(text, regex, tag_name):\n",
        "    phrases = []\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))):\n",
        "          if hasattr(chunk, 'label') and chunk.label() == tag_name:\n",
        "            phrase = \" \".join([c[0] for c in chunk])\n",
        "            if re.match(regex, phrase):\n",
        "                phrases.append(phrase)\n",
        "    return phrases"
      ],
      "metadata": {
        "id": "WAaU7aXwS2Oc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is Jonny Cage and I live in the swamp. My phone number is 511-592-887. I also like to eat pizza from Pizza from KFC.\"\n",
        "regex = r\".*\"\n",
        "tag_name = \"PERSON\"\n",
        "phrases = get_phrases(text, regex, tag_name)\n",
        "print(phrases)\n",
        "\n",
        "regex = r\"\\d{3}-\\d{3}-\\d{3}\"\n",
        "tag_name = None\n",
        "phrases = get_phrases(text, regex, tag_name)\n",
        "print(phrases)"
      ],
      "metadata": {
        "id": "_fDIadVSS62y",
        "outputId": "8f7c1c54-e6d7-4398-af3c-f1e85bc1bf43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jonny Cage']\n",
            "[]\n"
          ]
        }
      ]
    }
  ]
}